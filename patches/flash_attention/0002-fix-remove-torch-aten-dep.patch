From a1187750473ef112560c2d65a88438e58b2a5ec9 Mon Sep 17 00:00:00 2001
From: zw193905 <zw193905@alibaba-inc.com>
Date: Thu, 11 Jul 2024 10:23:22 +0800
Subject: [PATCH] fix - remove torch aten dep

---
 .../src/flash_fwd_launch_template.h           | 49 +++++++++++++------
 1 file changed, 33 insertions(+), 16 deletions(-)

diff --git csrc/flash_attn/src/flash_fwd_launch_template.h csrc/flash_attn/src/flash_fwd_launch_template.h
index e271ddd..30d6055 100644
--- csrc/flash_attn/src/flash_fwd_launch_template.h
+++ csrc/flash_attn/src/flash_fwd_launch_template.h
@@ -4,12 +4,32 @@

 #pragma once

-#include <ATen/cuda/CUDAContext.h>
-
 #include "static_switch.h"
 #include "flash.h"
 #include "flash_fwd_kernel.h"

+#define FA_CUDA_CHECK(EXPR)                                             \
+    do {                                                                \
+        const cudaError_t __err = EXPR;                                 \
+        if (__err) {                                                    \
+            throw std::runtime_error(std::string("FA CUDA runtime error: ") + (cudaGetErrorString(__err)) + " " \
+                                     + __FILE__ + ":" + std::to_string(__LINE__) + " \n"); \
+        }                                                               \
+    } while (0)
+
+#define FA_CUDA_KERNEL_LAUNCH_CHECK()  FA_CUDA_CHECK(cudaGetLastError())
+
+inline bool isSM8x() {
+    static bool IS_SM8X = [](){
+        int device;
+        FA_CUDA_CHECK(cudaGetDevice(&device));
+        cudaDeviceProp deviceProp;
+        FA_CUDA_CHECK(cudaGetDeviceProperties(&deviceProp, device));
+        return deviceProp.major == 8 && deviceProp.minor > 0;
+    }();
+    return IS_SM8X;
+}
+
 #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800) && (__CUDA_ARCH__ < 900)
 template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_local, bool Has_alibi, bool Is_even_MN, bool Is_even_K, bool Return_softmax>
 __global__ void flash_fwd_kernel(__grid_constant__ const Flash_fwd_params params) {
@@ -70,15 +90,15 @@ void run_flash_fwd(Flash_fwd_params &params, cudaStream_t stream) {
                         // printf("IsEvenMNConst = %d, IsEvenKConst = %d, Is_local = %d, Is_causal = %d, ReturnSoftmaxConst = %d, Is_dropout = %d\n", int(IsEvenMNConst), int(IsEvenKConst), int(Is_local), int(Is_causal), int(ReturnSoftmaxConst), int(Is_dropout));
                         // auto kernel = &flash_fwd_kernel<Kernel_traits, false, Is_causal, false, true, true, false>;
                         if (smem_size >= 48 * 1024) {
-                            C10_CUDA_CHECK(cudaFuncSetAttribute(
-                                kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
+                            FA_CUDA_CHECK(cudaFuncSetAttribute(
+                                                  kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
                         }
                         // int ctas_per_sm;
                         // cudaError status_ = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
                         //     &ctas_per_sm, kernel, Kernel_traits::kNThreads, smem_size);
                         // printf("smem_size = %d, CTAs per SM = %d\n", int(smem_size), ctas_per_sm);
                         kernel<<<grid, Kernel_traits::kNThreads, smem_size, stream>>>(params);
-                        C10_CUDA_KERNEL_LAUNCH_CHECK();
+                        FA_CUDA_KERNEL_LAUNCH_CHECK();
                     });
                 });
             });
@@ -109,11 +129,11 @@ void run_flash_splitkv_fwd(Flash_fwd_params &params, cudaStream_t stream) {
                                 // auto kernel = &flash_fwd_splitkv_kernel<Kernel_traits, Is_causal, false, true, Split, Append_KV>;
                                 // auto kernel = &flash_fwd_splitkv_kernel<Kernel_traits, Is_causal, false, IsEvenKConst>;
                                 if (smem_size >= 48 * 1024) {
-                                    C10_CUDA_CHECK(cudaFuncSetAttribute(
+                                    FA_CUDA_CHECK(cudaFuncSetAttribute(
                                         kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
                                 }
                                 kernel<<<grid, Kernel_traits::kNThreads, smem_size, stream>>>(params);
-                                C10_CUDA_KERNEL_LAUNCH_CHECK();
+                                FA_CUDA_KERNEL_LAUNCH_CHECK();
                             });
                         });
                     });
@@ -143,7 +163,7 @@ void run_flash_splitkv_fwd(Flash_fwd_params &params, cudaStream_t stream) {
             } else if (params.num_splits <= 128) {
                 flash_fwd_splitkv_combine_kernel<Kernel_traits, kBlockM, 7, IsEvenKConst><<<grid_combine, Kernel_traits::kNThreads, 0, stream>>>(params);
             }
-            C10_CUDA_KERNEL_LAUNCH_CHECK();
+            FA_CUDA_KERNEL_LAUNCH_CHECK();
         });
     }
 }
@@ -193,8 +213,7 @@ void run_mha_fwd_hdim64(Flash_fwd_params &params, cudaStream_t stream) {
 template<typename T>
 void run_mha_fwd_hdim96(Flash_fwd_params &params, cudaStream_t stream) {
     constexpr static int Headdim = 96;
-    auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm8x = dprops->major == 8 && dprops->minor > 0;
+    bool is_sm8x = isSM8x();
     DROPOUT_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
         BOOL_SWITCH(params.is_causal, Is_causal, [&] {
             // For sm86 or sm89, 64 x 64 is the fastest for causal (because it's square),
@@ -219,8 +238,7 @@ void run_mha_fwd_hdim96(Flash_fwd_params &params, cudaStream_t stream) {
 template<typename T>
 void run_mha_fwd_hdim128(Flash_fwd_params &params, cudaStream_t stream) {
     constexpr static int Headdim = 128;
-    auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm8x = dprops->major == 8 && dprops->minor > 0;
+    bool is_sm8x = isSM8x();
     DROPOUT_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
         BOOL_SWITCH(params.is_causal, Is_causal, [&] {
             if constexpr(!Is_dropout) {
@@ -256,8 +274,7 @@ void run_mha_fwd_hdim128(Flash_fwd_params &params, cudaStream_t stream) {
 template<typename T>
 void run_mha_fwd_hdim160(Flash_fwd_params &params, cudaStream_t stream) {
     constexpr static int Headdim = 160;
-    auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm8x = dprops->major == 8 && dprops->minor > 0;
+    bool is_sm8x = isSM8x();
     DROPOUT_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
         BOOL_SWITCH(params.is_causal, Is_causal, [&] {
             // For A100, H100, 128 x 32 is the fastest.
@@ -311,7 +328,7 @@ void run_mha_fwd_hdim224(Flash_fwd_params &params, cudaStream_t stream) {
     cudaError status_ = cudaDeviceGetAttribute(
         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);
     if (status_ != cudaSuccess) {
-      C10_CUDA_CHECK(status_);
+      FA_CUDA_CHECK(status_);
     }
     // printf("max_smem_per_block = %d\n", max_smem_per_block);
     DROPOUT_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
@@ -342,7 +359,7 @@ void run_mha_fwd_hdim256(Flash_fwd_params &params, cudaStream_t stream) {
     status_ = cudaDeviceGetAttribute(
         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);
     if (status_ != cudaSuccess) {
-      C10_CUDA_CHECK(status_);
+      FA_CUDA_CHECK(status_);
     }
     // printf("max_smem_per_sm = %d, max_smem_per_block = %d\n", max_smem_per_sm, max_smem_per_block);
     DROPOUT_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {
--
2.19.1.6.gb485710b
